model_path: Qwen/Qwen2.5-Math-1.5B
eval_suites: [all]  # options: gsm8k, lm_eval, all, hendrycks_math, mmlu, arc_challenge, hellaswag, winogrande, truthfulqa_mc2, wikitext
limit: null
output_dir: ./artifacts/eval

gsm8k_eval_path: artifacts/gsm8k/test.jsonl
gsm8k_max_tokens: 1536
gsm8k_k_shot: 0
gsm8k_bootstrap_samples: 1000  # costs little time/compute
gsm8k_ci_alpha: 0.05

lm_eval_tasks:
  - hendrycks_math
  - mmlu
  - arc_challenge
  - hellaswag
  - winogrande
  - truthfulqa_mc2
  - wikitext
lm_eval_fewshot: 4
lm_eval_batch_size: 8
lm_eval_max_tokens: 2048

# vLLM args
tp_size: 1
gpu_mem_util: 0.92
temperature: 0.0
top_p: 1.0  # match to train config

# Data-parallel for GSM8k & task-parallel for lm-eval
num_shards: 8  # should equal num available GPUs
base_port: 8000
