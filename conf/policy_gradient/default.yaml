train_data_path: artifacts/processed/train_templated.jsonl
val_data_path: artifacts/processed/val_templated.jsonl
model_id: Qwen/Qwen2.5-Math-1.5B
device: cuda:0
vllm_device: cuda:1
vllm_gpu_memory_utilization: 0.85
vllm_batch_prompts: 64  # vllm worker rolls out (this * group_size) sequences
group_size: 8
episodes_per_update: 512  # must be a multiple of group_size
soft_token_cap_per_update: 1000000
max_new_tokens: 2048  # generation length cap (action tokens)
temperature: 1.0
top_p: 1.0
# Optimizer / schedule
learning_rate: 1e-5
adamw_beta1: 0.9
adamw_beta2: 0.95
adamw_eps: 1e-8
weight_decay: 0.0
max_grad_norm: 1.0
total_update_steps: 200
# Policy gradient loss type
loss_type: grpo_clip
cliprange: 0.2
normalize_adv_by_std: true
advantage_eps: 1e-6
# Misc
trainer_microbatch_eps: 2  # how many episodes the trainer processes at once
eval_every: 20
eval_examples: null
checkpoint_dir: null  # full HF ckpt aligned to evals
model_dtype: null
resume_from: null
reward_fn: null  # null uses default
